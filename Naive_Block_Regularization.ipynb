{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcui/.conda/envs/auto-sklearn/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:17: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, defaultdict\n"
     ]
    }
   ],
   "source": [
    "# Surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import libraries\n",
    "from pmlb import dataset_names, classification_dataset_names, fetch_data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "\n",
    "# Import SK-learn and AutoSK-Learn\n",
    "import autosklearn.classification\n",
    "import autosklearn.regression\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usable_datasets = []\n",
    "dataset_min_count = 10\n",
    "dataset_max_count = 20\n",
    "count = 0\n",
    "\n",
    "for dataset in classification_dataset_names:\n",
    "    if count < dataset_min_count:\n",
    "        count += 1\n",
    "        continue\n",
    "    usable_datasets.append(dataset)\n",
    "    count += 1\n",
    "    if count >= dataset_max_count:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['allbp',\n",
       " 'allhyper',\n",
       " 'allhypo',\n",
       " 'allrep',\n",
       " 'analcatdata_aids',\n",
       " 'analcatdata_asbestos',\n",
       " 'analcatdata_authorship',\n",
       " 'analcatdata_bankruptcy',\n",
       " 'analcatdata_boxing1',\n",
       " 'analcatdata_boxing2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usable_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEW DATASET\n",
      "Score Logit Normal:  0.9618239660657476\n",
      "Score Logit Junk:  0.9671261930010604\n",
      "Naive Ensemble:  0.953340402969247\n",
      "[WARNING] [2019-02-28 00:21:05,428:EnsembleBuilder(1):e0ce6b46d04303af3ce7eaf24e477974] No models better than random - using Dummy Score!\n",
      "[WARNING] [2019-02-28 00:21:05,441:EnsembleBuilder(1):e0ce6b46d04303af3ce7eaf24e477974] No models better than random - using Dummy Score!\n",
      "[WARNING] [2019-02-28 00:21:06,044:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2019-02-28 00:21:06,044:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2019-02-28 00:21:07,452:EnsembleBuilder(1):e0ce6b46d04303af3ce7eaf24e477974] No models better than random - using Dummy Score!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "normal_logit_scores = []\n",
    "junk_logit_scores = []\n",
    "naive_ens_logit_scores = []\n",
    "automl_scores = []\n",
    "\n",
    "for dataset in usable_datasets:\n",
    "    print(\"\")\n",
    "    print(\"NEW DATASET\")\n",
    "    X, y = fetch_data(dataset, return_X_y=True)\n",
    "    \n",
    "    m, d = X.shape\n",
    "    \n",
    "    # Create and append junk data of feature size 200 (half discrete, half contintinous)\n",
    "    junk_discrete = np.round(np.random.rand(m, 100))\n",
    "    junk_continuous = np.random.rand(m, 100)\n",
    "    junk = np.hstack((junk_discrete, junk_continuous))\n",
    "    X_junk = np.hstack((X, junk))    \n",
    "    \n",
    "    logit_norm = LogisticRegression()\n",
    "    logit_junk = LogisticRegression()\n",
    "    \n",
    "    # Run models on the normal data and assess performance with regular ML models\n",
    "    train_X_reg, test_X_reg, train_y_reg, test_y_reg = train_test_split(X, y)\n",
    "    logit_norm.fit(train_X_reg, train_y_reg)\n",
    "    normal_logit_score = logit_norm.score(test_X_reg, test_y_reg)\n",
    "    normal_logit_scores.append(normal_logit_score)\n",
    "    print(\"Score Logit Normal: \", normal_logit_score)\n",
    "    \n",
    "    # Run models on the normal+junk data and assess performance with regular ML models\n",
    "    train_X_junk, test_X_junk, train_y_junk, test_y_junk = train_test_split(X_junk, y)\n",
    "    logit_junk.fit(train_X_junk, train_y_junk)\n",
    "    junk_logit_score = logit_junk.score(test_X_junk, test_y_junk)\n",
    "    junk_logit_scores.append(junk_logit_score)\n",
    "    print(\"Score Logit Junk: \", junk_logit_score)\n",
    "    \n",
    "    # Run naive ensembling with naive average with regular ML models\n",
    "    train_X_reg, test_X_reg, train_y, test_y = train_test_split(X, y, random_state=0)\n",
    "    train_X_junk, test_X_junk, train_y, test_y = train_test_split(junk, y, random_state=0)\n",
    "    \n",
    "    logit_p1 = LogisticRegression()\n",
    "    logit_p2 = LogisticRegression()\n",
    "    \n",
    "    logit_p1.fit(train_X_reg, train_y)\n",
    "    logit_p2.fit(train_X_junk, train_y)    \n",
    "    \n",
    "    pred1 = logit_p1.predict(test_X_reg)\n",
    "    pred2 = logit_p2.predict(test_X_junk)\n",
    "    \n",
    "    pred_y = np.round(0.5 * pred1 + 0.5 * pred2)\n",
    "    naive_ens_score = accuracy_score(pred_y, test_y)\n",
    "    print(\"Naive Ensemble: \", naive_ens_score)\n",
    "    naive_ens_logit_scores.append(naive_ens_score)\n",
    "                    \n",
    "    # Run Auto-SKLearn on the normal + junk data\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_junk, y)\n",
    "    automl = autosklearn.classification.AutoSklearnClassifier()\n",
    "    automl.fit(X_train, y_train)\n",
    "    y_hat = automl.predict(X_test)\n",
    "    automl_score = sklearn.metrics.accuracy_score(y_test, y_hat);\n",
    "    print(\"AutoML score: \", automl_score)\n",
    "    automl_scores.append(automl_score)\n",
    "    \n",
    "\n",
    "    # TODO: Run Block Regression\n",
    "    # TODO: Run Block Regression and Auto-SkLearn\n",
    "    \n",
    "    # TODO: Repeat this entire process but with different size junk groups and different numbers\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for normal logit:  0.849080506924536\n",
      "Average score for normal+junk logit:  0.8195549985171189\n",
      "Average score for naive ensemble with logit:  0.7824321681377665\n",
      "Average score for automl for normal+junk:  0.8349368658912667\n"
     ]
    }
   ],
   "source": [
    "print(\"Average score for normal logit: \", str(sum(normal_logit_scores)/float(len(normal_logit_scores))))\n",
    "print(\"Average score for normal+junk logit: \", str(sum(junk_logit_scores)/float(len(junk_logit_scores))))\n",
    "print(\"Average score for naive ensemble with logit: \", str(sum(naive_ens_logit_scores)/float(len(naive_ens_logit_scores))))\n",
    "print(\"Average score for automl for normal+junk: \", str(sum(automl_scores)/float(len(automl_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , ..., 0.40292058, 0.31297249,\n",
       "        0.4414766 ],\n",
       "       [0.        , 1.        , 0.        , ..., 0.12702129, 0.8205428 ,\n",
       "        0.86351832],\n",
       "       [0.        , 0.        , 0.        , ..., 0.71576606, 0.58220464,\n",
       "        0.57942425],\n",
       "       ...,\n",
       "       [2.        , 0.        , 0.        , ..., 0.47287437, 0.01392644,\n",
       "        0.63438904],\n",
       "       [1.        , 0.        , 0.        , ..., 0.42546666, 0.67785646,\n",
       "        0.18979169],\n",
       "       [1.        , 0.        , 0.        , ..., 0.78412312, 0.61925482,\n",
       "        0.15752893]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:auto-sklearn]",
   "language": "python",
   "name": "conda-env-auto-sklearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
