{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcui/.conda/envs/auto-sklearn/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:17: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, defaultdict\n"
     ]
    }
   ],
   "source": [
    "# Surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import libraries\n",
    "from pmlb import dataset_names, classification_dataset_names, regression_dataset_names, fetch_data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# Import SK-learn and AutoSK-Learn\n",
    "import autosklearn.classification\n",
    "import autosklearn.regression\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['-c', '--class_sets'], dest='class_sets', nargs=0, const=True, default=False, type=None, choices=None, help='Benchmark on classification sets (default)', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Run Auto-SkLearn on PMLB datasets')\n",
    "\n",
    "parser.add_argument('-min', '--minset', type=int, metavar='', required=False, default=1, help = 'Min dataset number (default 1)')\n",
    "parser.add_argument('-max', '--maxset', type=int, metavar='', required=False, default=166, help = '# Max dataset number (default is 166 for classifcation, 120 for regression)')\n",
    "parser.add_argument('-mem', '--memory', type=int, metavar='', required=False, default=3072, help = '# Memory capacity for the AutoSklean script (default 3072MB)')\n",
    "parser.add_argument('-noxg', '--no_xgboost', action='store_true', help = '# Remove XGBoost library from being used in Auto-SkLearn')\n",
    "parser.add_argument('-t', '--maxtime', type=int, metavar='', required=False, default=1, help = 'Maximum time to run the model for in seconds(default 3600)')\n",
    "\n",
    "class_group = parser.add_mutually_exclusive_group()\n",
    "class_group.add_argument('-r', '--regre_sets', action='store_true', help='Benchmark on regression sets')\n",
    "class_group.add_argument('-c', '--class_sets', action='store_true', help='Benchmark on classification sets (default)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = parser.parse_args(['-min','1','-max','2','-noxg','-t', '100', '-c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign variables based on arguments\n",
    "minset = args.minset\n",
    "maxset = args.maxset\n",
    "max_time = args.maxtime\n",
    "regre_sets = args.regre_sets\n",
    "class_sets = args.class_sets\n",
    "no_xgboost = args.no_xgboost\n",
    "memory_cap = args.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set classification sets to default if no class was selected\n",
    "if not regre_sets and not class_sets:\n",
    "    class_sets = True\n",
    "\n",
    "# Rescale dataset max number to be within boundaries\n",
    "if maxset < minset:\n",
    "    temp = maxset\n",
    "    maxset = minset\n",
    "    minset = temp\n",
    "if minset < 1:\n",
    "    minset = 1\n",
    "    print('Minset provided is less than 1, changed to 1.')\n",
    "if class_sets and maxset > 166:\n",
    "    maxset = 166\n",
    "    print('Maxset provided is greater than 166, changed to 166.')\n",
    "if regre_sets and maxset > 120:                \n",
    "    maxset = 120\n",
    "    print('Maxset provided is greater than 120, changed to 120.')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "100\n",
      "False\n",
      "True\n",
      "True\n",
      "3072\n"
     ]
    }
   ],
   "source": [
    "print(minset)\n",
    "print(maxset)\n",
    "print(max_time)\n",
    "print(regre_sets)\n",
    "print(class_sets)\n",
    "print(no_xgboost)\n",
    "print(memory_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of the number of features, instances, and classes per classification dataset\n",
    "# Potentially look into including number of binary, integer, and float features in the future\n",
    "\n",
    "datasets = []\n",
    "dataset_props = {}\n",
    "\n",
    "if class_sets:\n",
    "    dataset_names = classification_dataset_names[minset-1: maxset] \n",
    "if regre_sets:\n",
    "    dataset_names = regression_dataset_names[minset-1: maxset]\n",
    "\n",
    "dataset_number = minset;\n",
    "for dataset in dataset_names:\n",
    "    X, y = fetch_data(dataset, return_X_y=True)\n",
    "    num_instances, num_features =  X.shape\n",
    "    if num_instances > 500000:\n",
    "        dataset_number += 1\n",
    "        continue        \n",
    "    num_classes = (np.unique(y)).size if class_sets else -1\n",
    "    dataset_props[dataset] = (num_instances, num_features, num_classes, dataset_number)\n",
    "    dataset_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT TIME IS  64\n",
      "Auto-SKLearn, on dataset  GAMETES_Epistasis_2-Way_1000atts_0.4H_EDM-1_EDM-1_1  | Number:  1 max of  2\n",
      "Properties: \n",
      "(1600, 1000, 2, 1)\n",
      "Auto-SKLearn, fitting\n",
      "[WARNING] [2019-03-15 17:05:46,137:EnsembleBuilder(1):b7949df5eb6db3b08b85a9008850c8f2] No models better than random - using Dummy Score!\n",
      "[WARNING] [2019-03-15 17:05:46,147:EnsembleBuilder(1):b7949df5eb6db3b08b85a9008850c8f2] No models better than random - using Dummy Score!\n",
      "Time limit for a single run is higher than total time limit. Capping the limit for a single run to the total time given to SMAC (63.588802)\n",
      "Auto-SKLearn, testing\n",
      "Auto-SKLearn, finished testing on set  1\n",
      "Current time Autosklearn score:  0.4825\n",
      "size of model mb:  5.688225\n",
      "saved to  PMLB_benchmark_results/c_1_2_maxtime_100.csv\n",
      "Auto-SKLearn, on dataset  GAMETES_Epistasis_2-Way_20atts_0.1H_EDM-1_1  | Number:  2 max of  2\n",
      "Properties: \n",
      "(1600, 20, 2, 2)\n",
      "Auto-SKLearn, fitting\n",
      "[WARNING] [2019-03-15 17:06:45,139:EnsembleBuilder(1):ae4ce97e70e4c1eb50d3ce2b3bc89442] No models better than random - using Dummy Score!\n",
      "[WARNING] [2019-03-15 17:06:45,148:EnsembleBuilder(1):ae4ce97e70e4c1eb50d3ce2b3bc89442] No models better than random - using Dummy Score!\n",
      "Time limit for a single run is higher than total time limit. Capping the limit for a single run to the total time given to SMAC (63.603072)\n"
     ]
    }
   ],
   "source": [
    "# Add performance results of the datasets that we query on to a final dataframe to output\n",
    "df_rows_list = []\n",
    "time_cap = 64\n",
    "while(time_cap < max_time * 2):\n",
    "    # Cap the time cap at the max time when the benchmarker is on its final iteration\n",
    "    if(time_cap > max_time):\n",
    "        time_cap = max_time\n",
    "    print('CURRENT TIME IS ', time_cap)\n",
    "        \n",
    "    for dataset in dataset_names:\n",
    "        curr_dataset_results = {}\n",
    "        print(\"Auto-SKLearn, on dataset \", dataset, \" | Number: \", str(dataset_props[dataset][3]), \"max of \", str(maxset))\n",
    "        print(\"Properties: \")\n",
    "        print(str(dataset_props[dataset]))\n",
    "        \n",
    "        # Split the data to training and test sets\n",
    "        X, y = fetch_data(dataset, return_X_y=True)\n",
    "        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
    "\n",
    "        # Run the classifier\n",
    "        automl = 0;\n",
    "        \n",
    "        if class_sets:\n",
    "            if no_xgboost:\n",
    "                automl = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task = time_cap, \n",
    "                                                                          ml_memory_limit = memory_cap,\n",
    "                                                                          exclude_estimators = 'xgradient_boosting.py')\n",
    "            else:\n",
    "                automl = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task = time_cap, \n",
    "                                                                          ml_memory_limit = memory_cap)\n",
    "        if regre_sets:\n",
    "            if no_xgboost:\n",
    "                automl = autosklearn.regression.AutoSklearnRegressor(time_left_for_this_task = time_cap, \n",
    "                                                                 ml_memory_limit = memory_cap,\n",
    "                                                                 exclude_estimators = 'xgradient_boosting.py')                  \n",
    "            else:\n",
    "                automl = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task = time_cap, \n",
    "                                                                          ml_memory_limit = memory_cap)\n",
    "                \n",
    "        # Use the fit and test with AutoSkLearn on the current data.\n",
    "        # If exception occurs, continue to next dataset.\n",
    "        try:\n",
    "            print(\"Auto-SKLearn, fitting\")\n",
    "            automl.fit(X_train, y_train)\n",
    "            print(\"Auto-SKLearn, testing\")        \n",
    "            current_score = automl.score(X_test, y_test)                            \n",
    "            print(\"Auto-SKLearn, finished testing on set \", str(dataset_props[dataset][3]))\n",
    "            print(\"Current time Autosklearn score: \", str(current_score))\n",
    "        except:\n",
    "            print(\"EXCEPTION: CURRENT DATASET FAILED WITH AUTOSKLEARN. CONTINUING TO NEXT DATASET.\")\n",
    "            continue;\n",
    "                          \n",
    "        # Store the result in a dictionary\n",
    "        curr_dataset_results['name'] = dataset\n",
    "        curr_dataset_results['number'] = dataset_props[dataset][3]\n",
    "        curr_dataset_results['num_instances'] = dataset_props[dataset][0]\n",
    "        curr_dataset_results['num_features'] = dataset_props[dataset][1]\n",
    "        curr_dataset_results['num_classes'] = dataset_props[dataset][2]\n",
    "        curr_dataset_results['time_cap'] = time_cap\n",
    "        curr_dataset_results['score'] = current_score\n",
    "        \n",
    "        # Save the pickled model\n",
    "        curr_dataset_results['model'] = pickle.dumps(automl)\n",
    "        print('size of model mb: ', str(sys.getsizeof(curr_dataset_results['model'])/1000000))\n",
    "              \n",
    "        # Append current dictionary to a list of dictionary\n",
    "        df_rows_list.append(curr_dataset_results)                \n",
    "        \n",
    "        # Create a Pandas Dataframe with the results\n",
    "        autosklearn_df = pd.DataFrame(df_rows_list)\n",
    "        autosklearn_df.sort_values(by=['number', 'time_cap'])\n",
    "\n",
    "        # Save results into a CSV after every round\n",
    "        set_type_string = 'c' if class_sets else 'r'\n",
    "\n",
    "        file_name = 'PMLB_benchmark_results/' + set_type_string + '_' + str(minset) + '_' + str(maxset) + '_' + 'maxtime' + '_' + str(max_time) + '.csv'\n",
    "        print('saved to ', file_name)\n",
    "\n",
    "        autosklearn_df.to_csv(file_name, sep='\\t')\n",
    "        \n",
    "    time_cap *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:auto-sklearn]",
   "language": "python",
   "name": "conda-env-auto-sklearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
